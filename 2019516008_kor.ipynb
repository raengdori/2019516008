{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"bert_kor_org.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e4c708f602cc442eba0f3561e62b5859":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cadfec02730b4b1eb4f9dd3c64d82664","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fc98015df2ec4974947393c5b1548245","IPY_MODEL_4e7ec2b4e2e5420a8ada9b4672cf58c0"]}},"cadfec02730b4b1eb4f9dd3c64d82664":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc98015df2ec4974947393c5b1548245":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5b6f281c2721445ebad91af603ca13fa","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a1a0ff4cf80941c69eca25fe858a0edd"}},"4e7ec2b4e2e5420a8ada9b4672cf58c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f9b18f2f746044dd9dd4582f1bf0e49c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 996k/996k [00:09&lt;00:00, 103kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3479c22053c645789719f094d5423d6e"}},"5b6f281c2721445ebad91af603ca13fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a1a0ff4cf80941c69eca25fe858a0edd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9b18f2f746044dd9dd4582f1bf0e49c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3479c22053c645789719f094d5423d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"251ebc0697874ca49074c96e41d0643f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4b26f8f7cb964f2fa7da7bb20cb032f8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9d1a9ef29bd24f2c940884ba879bc71b","IPY_MODEL_9a3c310780b4461bafff4f3b5c923edc"]}},"4b26f8f7cb964f2fa7da7bb20cb032f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9d1a9ef29bd24f2c940884ba879bc71b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_537103124edc456395e4ab1863079384","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61759a8ab5064aa388a1ed8adaf8b3f6"}},"9a3c310780b4461bafff4f3b5c923edc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1392448e92f54a56ad2daf67c3830ef4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:55&lt;00:00, 11.3B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61be335f0fdd433e94be4729fe96f54b"}},"537103124edc456395e4ab1863079384":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"61759a8ab5064aa388a1ed8adaf8b3f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1392448e92f54a56ad2daf67c3830ef4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"61be335f0fdd433e94be4729fe96f54b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8274d75674eb4fcbabd94918adbd4a8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bcb43b58b1704687b71deee70838f1f7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_455b2c371c5347539937cd439827c838","IPY_MODEL_f8c6f0c0d4564f29bcd86880993c2150"]}},"bcb43b58b1704687b71deee70838f1f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"455b2c371c5347539937cd439827c838":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9dccf8c47dee42e9b1f48fdde9e826bc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d28528087f71418a901bf46701b34f6b"}},"f8c6f0c0d4564f29bcd86880993c2150":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_deb9c9258cb143bcaf8070e9ec6744c4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 714M/714M [00:54&lt;00:00, 13.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2e2eba4295884987ad5f5ee785c644de"}},"9dccf8c47dee42e9b1f48fdde9e826bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d28528087f71418a901bf46701b34f6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"deb9c9258cb143bcaf8070e9ec6744c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2e2eba4295884987ad5f5ee785c644de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plsGIkLv-53m","executionInfo":{"status":"ok","timestamp":1608600830470,"user_tz":-540,"elapsed":9868,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"af33a781-a429-4a29-f623-c5f143a5efd6"},"source":["# Hugging Face의 트랜스포머 모델을 설치\n","!pip install transformers konlpy koalanlp\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 8.9MB/s \n","\u001b[?25hCollecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n","\u001b[?25hCollecting koalanlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/69/a4aa4db5b231ee48bd3079a3a683a4e3d2e9f6cd45d4e024c1dc78237642/koalanlp-2.1.6-py3-none-any.whl (49kB)\n","\u001b[K     |████████████████████████████████| 51kB 9.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 48.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 61.2MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n","\u001b[K     |████████████████████████████████| 460kB 51.4MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Collecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Collecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 13.3MB/s \n","\u001b[?25hCollecting py4j~=0.10\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/42/25ad191f311fcdb38b750d49de167abd535e37a144e730a80d7c439d1751/py4j-0.10.9.1-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 55.9MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0f9cb392256bca29c78c33a31743bef98a9caf3b57caa91e11ef54710002de0a\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers, JPype1, tweepy, colorama, beautifulsoup4, konlpy, py4j, koalanlp\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 koalanlp-2.1.6 konlpy-0.5.2 py4j-0.10.9.1 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1 tweepy-3.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVznZT-imWQc","executionInfo":{"status":"ok","timestamp":1608600713142,"user_tz":-540,"elapsed":21508,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"0adea2b6-567c-41e9-bc51-a41813adc77c"},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"unf7P41V_Bbg","executionInfo":{"status":"ok","timestamp":1608600847164,"user_tz":-540,"elapsed":1035,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["import tensorflow as tf\r\n","import torch\r\n","\r\n","from transformers import BertTokenizer\r\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n","from transformers import get_linear_schedule_with_warmup\r\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import random\r\n","import time\r\n","import datetime\r\n","from konlpy.tag import Okt\r\n","import pickle\r\n","from koalanlp import API\r\n","from koalanlp.proc import EntityRecognizer\r\n","from koalanlp.Util import initialize, finalize\r\n","import os"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"1epTdHc7_NSh"},"source":["\r\n","# 네이버 영화리뷰 감정분석 데이터 다운로드\r\n","!git clone https://github.com/e9t/nsmc.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jjp1Kb5M-535","executionInfo":{"status":"ok","timestamp":1608600861411,"user_tz":-540,"elapsed":972,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"f11ead67-5a12-4f36-8a97-cd154723c6a3"},"source":["# 판다스로 훈련셋과 테스트셋 데이터 로드\n","train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n","test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')\n","\n","print(train.shape)\n","print(test.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(150000, 3)\n","(50000, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b3-IPBUp-539","executionInfo":{"status":"ok","timestamp":1608600865510,"user_tz":-540,"elapsed":874,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["sentences = train['document']"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-r8feaP-53-","executionInfo":{"status":"ok","timestamp":1608600867863,"user_tz":-540,"elapsed":973,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIdMfCcj-53_","executionInfo":{"status":"ok","timestamp":1608600870131,"user_tz":-540,"elapsed":865,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 라벨 추출\n","labels = train['label'].values"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["e4c708f602cc442eba0f3561e62b5859","cadfec02730b4b1eb4f9dd3c64d82664","fc98015df2ec4974947393c5b1548245","4e7ec2b4e2e5420a8ada9b4672cf58c0","5b6f281c2721445ebad91af603ca13fa","a1a0ff4cf80941c69eca25fe858a0edd","f9b18f2f746044dd9dd4582f1bf0e49c","3479c22053c645789719f094d5423d6e"]},"id":"YJwK9C_V-54B","executionInfo":{"status":"ok","timestamp":1608600897572,"user_tz":-540,"elapsed":25078,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"4a1dc6e0-1418-4698-f997-0dcafc747bfe"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[filelock] Lock 140432175438312 acquired on /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4c708f602cc442eba0f3561e62b5859","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[filelock] Lock 140432175438312 released on /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u6mqbtcX-54C","executionInfo":{"status":"ok","timestamp":1608601018496,"user_tz":-540,"elapsed":5394,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxlsaXlA-54D","executionInfo":{"status":"ok","timestamp":1608601030945,"user_tz":-540,"elapsed":11771,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gPeThcG-54E","executionInfo":{"status":"ok","timestamp":1608601034360,"user_tz":-540,"elapsed":2456,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tpl-mL91-54G","executionInfo":{"status":"ok","timestamp":1608601036225,"user_tz":-540,"elapsed":946,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"acNKYCU_-54H","executionInfo":{"status":"ok","timestamp":1608601041041,"user_tz":-540,"elapsed":1039,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["##전처리 테스트셋"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"m9C0Bz9--54H","executionInfo":{"status":"ok","timestamp":1608601043040,"user_tz":-540,"elapsed":959,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 리뷰 문장 추출\n","sentences = test['document']"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_nXwLvM-54I","executionInfo":{"status":"ok","timestamp":1608601044843,"user_tz":-540,"elapsed":993,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VJNncYc-54J","executionInfo":{"status":"ok","timestamp":1608601046195,"user_tz":-540,"elapsed":785,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 라벨 추출\n","labels = test['label'].values"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKAZjn6u-54J","executionInfo":{"status":"ok","timestamp":1608601056575,"user_tz":-540,"elapsed":9155,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5J0OoO7-54K","executionInfo":{"status":"ok","timestamp":1608601060052,"user_tz":-540,"elapsed":2188,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"Np_A5BjO-54L","executionInfo":{"status":"ok","timestamp":1608601069322,"user_tz":-540,"elapsed":5007,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCt43rJB-54M","executionInfo":{"status":"ok","timestamp":1608601071559,"user_tz":-540,"elapsed":1209,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["\n","# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"WNA3cMdP-54M","executionInfo":{"status":"ok","timestamp":1608601073278,"user_tz":-540,"elapsed":936,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mn1C4_4K-54N","executionInfo":{"status":"ok","timestamp":1608601083310,"user_tz":-540,"elapsed":6198,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"1eb1caed-74aa-4179-9659-67da7ad88597"},"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bv1a9rYK-54O","executionInfo":{"status":"ok","timestamp":1608601084803,"user_tz":-540,"elapsed":669,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"6e61a4f4-34a7-4617-a9d1-eb8c5a5ce44b"},"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["251ebc0697874ca49074c96e41d0643f","4b26f8f7cb964f2fa7da7bb20cb032f8","9d1a9ef29bd24f2c940884ba879bc71b","9a3c310780b4461bafff4f3b5c923edc","537103124edc456395e4ab1863079384","61759a8ab5064aa388a1ed8adaf8b3f6","1392448e92f54a56ad2daf67c3830ef4","61be335f0fdd433e94be4729fe96f54b","8274d75674eb4fcbabd94918adbd4a8e","bcb43b58b1704687b71deee70838f1f7","455b2c371c5347539937cd439827c838","f8c6f0c0d4564f29bcd86880993c2150","9dccf8c47dee42e9b1f48fdde9e826bc","d28528087f71418a901bf46701b34f6b","deb9c9258cb143bcaf8070e9ec6744c4","2e2eba4295884987ad5f5ee785c644de"]},"id":"_Wo_wJGs-54P","executionInfo":{"status":"ok","timestamp":1608601159225,"user_tz":-540,"elapsed":71968,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"26ba2e52-d6a1-4ae6-c6d4-08664faf96b7"},"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n","model.cuda()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["[filelock] Lock 140432175436296 acquired on /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"251ebc0697874ca49074c96e41d0643f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[filelock] Lock 140432175436296 released on /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["[filelock] Lock 140431887601560 acquired on /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8274d75674eb4fcbabd94918adbd4a8e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[filelock] Lock 140431887601560 released on /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"p5ml4td3-54P","executionInfo":{"status":"ok","timestamp":1608602019335,"user_tz":-540,"elapsed":768,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 5\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"PH7z8WsP-54Q","executionInfo":{"status":"ok","timestamp":1608602021335,"user_tz":-540,"elapsed":847,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIHLA40h-54R","executionInfo":{"status":"ok","timestamp":1608602026635,"user_tz":-540,"elapsed":791,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMHIGL5Y-54S","executionInfo":{"status":"ok","timestamp":1608617570769,"user_tz":-540,"elapsed":15541327,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"230c0fa4-bec8-4320-a670-989ab7f7ac2b"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":35,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 5 ========\n","Training...\n","  Batch   500  of  4,219.    Elapsed: 0:05:54.\n","  Batch 1,000  of  4,219.    Elapsed: 0:11:49.\n","  Batch 1,500  of  4,219.    Elapsed: 0:17:44.\n","  Batch 2,000  of  4,219.    Elapsed: 0:23:39.\n","  Batch 2,500  of  4,219.    Elapsed: 0:29:34.\n","  Batch 3,000  of  4,219.    Elapsed: 0:35:29.\n","  Batch 3,500  of  4,219.    Elapsed: 0:41:24.\n","  Batch 4,000  of  4,219.    Elapsed: 0:47:19.\n","\n","  Average training loss: 0.39\n","  Training epcoh took: 0:49:54\n","\n","Running Validation...\n","  Accuracy: 0.86\n","  Validation took: 0:01:54\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","  Batch   500  of  4,219.    Elapsed: 0:05:55.\n","  Batch 1,000  of  4,219.    Elapsed: 0:11:49.\n","  Batch 1,500  of  4,219.    Elapsed: 0:17:44.\n","  Batch 2,000  of  4,219.    Elapsed: 0:23:39.\n","  Batch 2,500  of  4,219.    Elapsed: 0:29:35.\n","  Batch 3,000  of  4,219.    Elapsed: 0:35:29.\n","  Batch 3,500  of  4,219.    Elapsed: 0:41:24.\n","  Batch 4,000  of  4,219.    Elapsed: 0:47:20.\n","\n","  Average training loss: 0.29\n","  Training epcoh took: 0:49:55\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation took: 0:01:54\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","  Batch   500  of  4,219.    Elapsed: 0:05:55.\n","  Batch 1,000  of  4,219.    Elapsed: 0:11:50.\n","  Batch 1,500  of  4,219.    Elapsed: 0:17:45.\n","  Batch 2,000  of  4,219.    Elapsed: 0:23:40.\n","  Batch 2,500  of  4,219.    Elapsed: 0:29:34.\n","  Batch 3,000  of  4,219.    Elapsed: 0:35:29.\n","  Batch 3,500  of  4,219.    Elapsed: 0:41:24.\n","  Batch 4,000  of  4,219.    Elapsed: 0:47:19.\n","\n","  Average training loss: 0.23\n","  Training epcoh took: 0:49:54\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation took: 0:01:54\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","  Batch   500  of  4,219.    Elapsed: 0:05:55.\n","  Batch 1,000  of  4,219.    Elapsed: 0:11:50.\n","  Batch 1,500  of  4,219.    Elapsed: 0:17:45.\n","  Batch 2,000  of  4,219.    Elapsed: 0:23:40.\n","  Batch 2,500  of  4,219.    Elapsed: 0:29:35.\n","  Batch 3,000  of  4,219.    Elapsed: 0:35:30.\n","  Batch 3,500  of  4,219.    Elapsed: 0:41:25.\n","  Batch 4,000  of  4,219.    Elapsed: 0:47:20.\n","\n","  Average training loss: 0.18\n","  Training epcoh took: 0:49:56\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation took: 0:01:54\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","  Batch   500  of  4,219.    Elapsed: 0:05:55.\n","  Batch 1,000  of  4,219.    Elapsed: 0:11:50.\n","  Batch 1,500  of  4,219.    Elapsed: 0:17:44.\n","  Batch 2,000  of  4,219.    Elapsed: 0:23:39.\n","  Batch 2,500  of  4,219.    Elapsed: 0:29:34.\n","  Batch 3,000  of  4,219.    Elapsed: 0:35:28.\n","  Batch 3,500  of  4,219.    Elapsed: 0:41:23.\n","  Batch 4,000  of  4,219.    Elapsed: 0:47:18.\n","\n","  Average training loss: 0.15\n","  Training epcoh took: 0:49:53\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation took: 0:01:54\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7kicBzw-54U","executionInfo":{"status":"ok","timestamp":1608617951314,"user_tz":-540,"elapsed":380523,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"a7abd33a-f713-4608-c1f8-020435eb0dc1"},"source":["#테스트셋평가\n","#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["  Batch   100  of  1,563.    Elapsed: 0:00:24.\n","  Batch   200  of  1,563.    Elapsed: 0:00:48.\n","  Batch   300  of  1,563.    Elapsed: 0:01:13.\n","  Batch   400  of  1,563.    Elapsed: 0:01:37.\n","  Batch   500  of  1,563.    Elapsed: 0:02:01.\n","  Batch   600  of  1,563.    Elapsed: 0:02:26.\n","  Batch   700  of  1,563.    Elapsed: 0:02:50.\n","  Batch   800  of  1,563.    Elapsed: 0:03:14.\n","  Batch   900  of  1,563.    Elapsed: 0:03:38.\n","  Batch 1,000  of  1,563.    Elapsed: 0:04:03.\n","  Batch 1,100  of  1,563.    Elapsed: 0:04:27.\n","  Batch 1,200  of  1,563.    Elapsed: 0:04:51.\n","  Batch 1,300  of  1,563.    Elapsed: 0:05:15.\n","  Batch 1,400  of  1,563.    Elapsed: 0:05:40.\n","  Batch 1,500  of  1,563.    Elapsed: 0:06:04.\n","\n","Accuracy: 0.87\n","Test took: 0:06:19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jYZTnV-L-54W","executionInfo":{"status":"ok","timestamp":1608617951315,"user_tz":-540,"elapsed":380519,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["#새로운 문장테스트\n","# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"-jtsnK-C-54Z","executionInfo":{"status":"ok","timestamp":1608617951316,"user_tz":-540,"elapsed":380518,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UEF0ZCv-54a","executionInfo":{"status":"ok","timestamp":1608617951317,"user_tz":-540,"elapsed":380509,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"0ef72eeb-1886-4e71-9b37-1aedd9c26df6"},"source":["logits = test_sentences(['연기는 별로지만 재미 하나는 끝내줌!'])\n","\n","print(logits)\n","print(np.argmax(logits))\n","print(np.argmax(logits))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["[[-2.6246474  2.5011108]]\n","1\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"js2UOquZ-54d","executionInfo":{"status":"ok","timestamp":1608617951318,"user_tz":-540,"elapsed":380505,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}},"outputId":"3d1cf04d-2d0c-4214-dfe6-1372d2bff419"},"source":["logits = test_sentences(['재밌어요'])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["[[-0.06869861  0.3346338 ]]\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oSTdkLP8-54e","executionInfo":{"status":"ok","timestamp":1608619012182,"user_tz":-540,"elapsed":1075,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["predict=[]\r\n","#본인 구글 드라이브의 폴더에 미리 ko_data.csv(캐글 데이터)를  넣어놓고 해당 경로 명시\r\n","leader_test = pd.read_csv(\"/content/drive/MyDrive/ko_data.csv\",encoding='CP949')\r\n","leader_test= leader_test.rename(columns={'Sentence':'document'})"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"7On3-NZ1-hc4","executionInfo":{"status":"ok","timestamp":1608619164098,"user_tz":-540,"elapsed":144398,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["for sentence in leader_test['document']:\r\n","  logits = test_sentences([sentence])\r\n","  predict.append(np.argmax(logits))"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPPy3hrPrvg5","executionInfo":{"status":"ok","timestamp":1608619332152,"user_tz":-540,"elapsed":768,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":["import csv\r\n","f = open('/content/drive/MyDrive/sample.csv','a', newline='')\r\n","wr = csv.writer(f)\r\n","wr.writerow(['Id','Predicted'])\r\n","for i in range(len(predict)):\r\n","  wr.writerow([i,predict[i]])\r\n","f.close()"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"cS6lNVEf9iyG","executionInfo":{"status":"aborted","timestamp":1608619164100,"user_tz":-540,"elapsed":133322,"user":{"displayName":"KIM NaRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNB3_cUKk3YCnOzsiFvYVHfb24zO1pUepw0BHoOfk=s64","userId":"16971122573477489496"}}},"source":[""],"execution_count":null,"outputs":[]}]}